---
title: "PartIID"
author: "Hua Ji"
date: "4/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Load packages
The `tidyverse` and `caret` packages are loaded below.

```{r, load_packages}
library(tidyverse)
library(caret)
```

## Read and prepare data

The code chunk below reads in the `df_all` data and displays 

```{r, read_data}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)
df_all %>%glimpse()
```

## Added log-transformed response

```{r}
log_all<-df_all %>%mutate(logresponse =log(response))
log_all%>%glimpse()
```


## Resampling and performance metrics
caret will manage the training, tuning, and assessment of the models. 
Resampling and performance metrics
caret will manage the training, tuning, and assessment of the models. We must specify a resampling scheme and a primary performance metric. Letâ€™s use 5-fold cross-validation with 3-repeats. Our primary performance metric will be RMSE.

```{r}
my_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 3,savePredictions = TRUE)

my_metric <- "RMSE"
```
##1.Linear methods
##1a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_lm_1a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_1a

```

```{r}
fit_lm_1a$pred
```

##1b)All pairwise interactions of continuous inputs, include additive categorical features
```{r}
fit_lm_1b <- train(logresponse~(.-outcome-response-rowid-region-customer)^2+(region+customer),
                     
                 data = log_all ,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_1b

```
##1c)Interact region with continuous inputs, do not include customer
```{r}
fit_lm_1c <- train( logresponse ~ region * (.-region), 
                     
                 data = log_all %>% select(!c("rowid", "customer", "response","outcome")),
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_1c
```


```{r}
fit_lm_1c %>% readr::write_rds('fit_lm_1c.rds')
```

##1d)Interact region with customer, do not include customer
```{r}
fit_lm_1d <- train( logresponse ~ region * customer, 
                 data = log_all,
                  method = "lm",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_lm_1d
```

##2.Regularized regression with Elastic net
##2a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_enet_2a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "glmnet",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_enet_2a

```
##2b)All pairwise interactions of continuous inputs, include additive categorical features
```{r}
fit_enet_2b <- train(logresponse~(.-outcome-response-rowid-region-customer)^2+(region+customer),
                     
                 data = log_all ,
                  method = "glmnet",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_enet_2b

```
##2c)Interact region with continuous inputs, do not include customer
```{r}
fit_enet_2c <- train( logresponse ~ region * (.-region), 
                     
                 data = log_all %>% select(!c("rowid", "customer", "response","outcome")),
                  method = "glmnet",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_enet_2c
```


##3.Neural network
##3a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_nnet_3a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "nnet",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl,
                  trace = FALSE,
                  linout = TRUE)

fit_nnet_3a

```

##4.Random forest
##4a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_rf_4a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "rf",
                  metric = my_metric,
                  trControl = my_ctrl,
                  importance =TRUE)

fit_rf_4a

```


##5.Gradient boosted tree
##5a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_xgb_5a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "xgbTree",
                  metric = my_metric,
                  trControl = my_ctrl,
                  objective = 'reg:squarederror')

fit_xgb_5a

```


##6.Support Vector Machines (SVM)
##6a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_svm_6a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "svmRadial",
                  metric = my_metric,
                  preProcess = c("center", "scale"),
                  trControl = my_ctrl)

fit_svm_6a

```

##7.Decision tree
##7a)All categorical and continuous inputs - linear additive features

```{r}
set.seed(2021)

fit_rpart_7a <- train(logresponse~.-rowid-response-outcome,
                  data =log_all,
                  method = "rpart",
                  metric = my_metric,
                  trControl = my_ctrl)

fit_rpart_7a

```


###Compare models
##Compile the resampling results together.

Because the LM_2 has too large rmse value, so we will remove it to see the trend.

```{r, solution_05a, eval=TRUE}

my_results <- resamples(list(LM_1 = fit_lm_1a,
                             LM_3 = fit_lm_1c,
                             LM_4 = fit_lm_1d,
                             ENET_1 = fit_enet_2a,
                             ENET_2 = fit_enet_2b,
                             ENET_3 = fit_enet_2c,
                             NNET = fit_nnet_3a,
                             RF = fit_rf_4a,
                             XGB = fit_xgb_5a,
                             SVM = fit_svm_6a,
                             CART = fit_rpart_7a))
```

Compare models based on RMSE.
```{r}
dotplot(my_results)

```

According to the results of MAE, RMSE and Rsquared,  I will choose SVM as the best model.