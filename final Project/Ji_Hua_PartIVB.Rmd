---
title: "PartIVB"
author: "Hua Ji"
date: "4/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, load_tidy_package}
library(tidyverse)
```

```{r, load_caret_package}
library(caret)
```

## Read training data

The training data set is read in the code chunk below assuming you have downloaded the data from Canvas.  

```{r, read_data_01}
df_all <- readr::read_csv("final_project_train.csv", col_names = TRUE)
```

## Regression problem

The data associated with the regression task is created below. Note that the log-transformed response is calculated and assigned to the variable `y`.  

```{r, reg_01}
dfii <- df_all %>% 
  mutate(y = log(response)) %>% 
  select(region, customer, starts_with('x'), y)
```

```{r, reg_02}
my_ctrl_regress <- trainControl(method = 'repeatedcv', number = 5, repeats = 3, savePredictions = TRUE)
```

Next, define the primary performance metric of the model.  

```{r, reg_03}
my_metrics_regress <- 'RMSE'
```


```{r, reg_04a}
my_regress_method <- 'svmRadial'
```
  

```{r, reg_04}
set.seed(2022)
fit_svm <- train (y~.,
                     data = dfii,
                     method = my_regress_method,
                     preProcess = c("center", "scale"),
                     metric = my_metrics_regress,
                     trControl = my_ctrl_regress)
```


```{r, reg_05}
fit_svm
```


```{r}
resample_pred_classification_1<-fit_svm$pred%>%
  left_join(dfii%>%tibble::rowid_to_column("rowIndex"), by="rowIndex")
```

```{r}
for(customer in c("A","B","D","E","G","K","M","other","Q")){           
  small_table=resample_pred_classification_1[which(resample_pred_classification_1$customer==customer),]
  rmse_value=RMSE(small_table$obs,small_table$pred)
  print(rmse_value)
}
```

According to the RMSE results, the greater the RMSE value, the harder the customer is to predict. So, we can see that customer K is the most hard customer to predict.

## Classification problem

```{r, class_01}
dfiiiD <- df_all %>% 
  mutate(outcome = factor(outcome, 
                          levels = c("event", "non_event"))) %>% 
  select(region, customer, starts_with('x'), outcome)
```


### Accuracy

The code chunk below specifies the resampling scheme that we will use for the model associated with the Accuracy metric.  

```{r, acc_01}
my_ctrl_acc <- trainControl(method = 'repeatedcv', number = 5, repeats = 3, savePredictions = TRUE,summaryFunction = twoClassSummary,classProbs = TRUE)
```

Next, define the primary performance metric.  

```{r, acc_02}
my_metrics_acc <- "ROC"
```


```{r, acc_03}
my_binary_method <- 'glmnet'
```



```{r, acc_04}
set.seed(2022)
       fit_Enet<- train(outcome ~ .,
                        data = dfiiiD,
                        method = my_binary_method,
                        preProcess = c('center', 'scale'),
                        metric = my_metrics_acc,
                        trControl = my_ctrl_acc
                        )
```


```{r, acc_05}
fit_Enet
```

```{r}
library(yardstick)
```

```{r}
resample_pred_classification<-fit_Enet$pred%>%
  left_join(dfiiiD%>%tibble::rowid_to_column("rowIndex"), by="rowIndex")
```

```{r}
for(customer in c("A","B","D","E","G","K","M","other","Q")){
  small_table=resample_pred_classification[which(resample_pred_classification$customer==customer),]
  roc_value =small_table%>%roc_auc(obs,event)
  print(roc_value$.estimate)
}
```

According to the classification ROC results, we can see that the smaller the ROC value is,the harder the customer is to predict. So customer B should be the hardest to predictin classification model.